{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65780482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\solar-data-tools\\lib\\contextlib.py:142: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources on AWS. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "# Please add your stuff before running this demo (see the comments for details)\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_cloudprovider.aws import FargateCluster\n",
    "r'''\n",
    "Optional:\n",
    "    tags:  only use this if your organization enforces tag policies\n",
    "Required:\n",
    "    image:   should be a dockerhub public image. Please customize your image if needed\n",
    "'''\n",
    "tags = {\"project-pa-number\": \"VALUE_HERE\", \"project\": \"VALUE_HERE\"}\n",
    "cluster = FargateCluster(tags=tags, image=\"jjcrush/dask:latest\")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41cf26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b1db81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scheduler': {'host': {'python': '3.10.13.final.0',\n",
       "   'python-bits': 64,\n",
       "   'OS': 'Linux',\n",
       "   'OS-release': '5.10.198-187.748.amzn2.x86_64',\n",
       "   'machine': 'x86_64',\n",
       "   'processor': '',\n",
       "   'byteorder': 'little',\n",
       "   'LC_ALL': 'None',\n",
       "   'LANG': 'C.UTF-8'},\n",
       "  'packages': {'python': '3.10.13.final.0',\n",
       "   'dask': '2023.11.0',\n",
       "   'distributed': '2023.11.0',\n",
       "   'msgpack': '1.0.7',\n",
       "   'cloudpickle': '3.0.0',\n",
       "   'tornado': '6.3.3',\n",
       "   'toolz': '0.12.0',\n",
       "   'numpy': '1.26.2',\n",
       "   'pandas': '2.1.3',\n",
       "   'lz4': '4.3.2'}},\n",
       " 'workers': {},\n",
       " 'client': {'host': {'python': '3.10.13.final.0',\n",
       "   'python-bits': 64,\n",
       "   'OS': 'Windows',\n",
       "   'OS-release': '10',\n",
       "   'machine': 'AMD64',\n",
       "   'processor': 'AMD64 Family 25 Model 80 Stepping 0, AuthenticAMD',\n",
       "   'byteorder': 'little',\n",
       "   'LC_ALL': 'None',\n",
       "   'LANG': 'None'},\n",
       "  'packages': {'python': '3.10.13.final.0',\n",
       "   'dask': '2023.11.0',\n",
       "   'distributed': '2023.11.0',\n",
       "   'msgpack': '1.0.7',\n",
       "   'cloudpickle': '3.0.0',\n",
       "   'tornado': '6.3.3',\n",
       "   'toolz': '0.12.0',\n",
       "   'numpy': '1.26.2',\n",
       "   'pandas': '2.1.3',\n",
       "   'lz4': '4.3.2'}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_versions(check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd4af297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from solardatatools import DataHandler\n",
    "from solardatatools.dataio import get_pvdaq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8adfebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...queries complete in 1.5 seconds       \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is just an example line. We are deprecating Cassandra, this will be replaced in the future.\n",
    "# Please replace it with your own GET_DATA api\n",
    "df = get_pvdaq_data(sysid=34, year=2011, api_key='DEMO_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "656878cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\solardatatools\\time_axis_manipulation.py:146: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if avg_day[0] >= thresh:\n",
      "d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\solardatatools\\time_axis_manipulation.py:152: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  if avg_day[-1] >= thresh:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 13.36 seconds\n",
      "--------------------------------\n",
      "Breakdown\n",
      "--------------------------------\n",
      "Preprocessing              1.54s\n",
      "Cleaning                   0.07s\n",
      "Filtering/Summarizing      11.74s\n",
      "    Data quality           0.05s\n",
      "    Clear day detect       0.09s\n",
      "    Clipping detect        9.86s\n",
      "    Capacity change detect 1.75s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dh = DataHandler(df)\n",
    "dh.run_pipeline(power_col='ac_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68059e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of parallel computation within a cluster. \n",
    "# It handles run_pipeline() in a parallel way.\n",
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "def run_pipeline(df):\n",
    "    dh = DataHandler(df)\n",
    "    dh.run_pipeline(power_col='ac_power')\n",
    "    return dh.report(return_values=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dbfe8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'length': 1.0,\n",
       "   'capacity': 109.3,\n",
       "   'sampling': 15,\n",
       "   'quality score': 0.9835616438356164,\n",
       "   'clearness score': 0.4986301369863014,\n",
       "   'inverter clipping': True,\n",
       "   'clipped fraction': 0.01643835616438356,\n",
       "   'capacity change': False,\n",
       "   'data quality warning': False,\n",
       "   'time shift correction': False,\n",
       "   'time zone correction': 0},\n",
       "  {'length': 1.0,\n",
       "   'capacity': 109.3,\n",
       "   'sampling': 15,\n",
       "   'quality score': 0.9835616438356164,\n",
       "   'clearness score': 0.4986301369863014,\n",
       "   'inverter clipping': True,\n",
       "   'clipped fraction': 0.01643835616438356,\n",
       "   'capacity change': False,\n",
       "   'data quality warning': False,\n",
       "   'time shift correction': False,\n",
       "   'time zone correction': 0},\n",
       "  {'length': 1.0,\n",
       "   'capacity': 109.3,\n",
       "   'sampling': 15,\n",
       "   'quality score': 0.9835616438356164,\n",
       "   'clearness score': 0.4986301369863014,\n",
       "   'inverter clipping': True,\n",
       "   'clipped fraction': 0.01643835616438356,\n",
       "   'capacity change': False,\n",
       "   'data quality warning': False,\n",
       "   'time shift correction': False,\n",
       "   'time zone correction': 0}],)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 19:43:21,830 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x000001A01CEB1750>>, <Task finished name='Task-14260' coro=<SpecCluster._correct_state_internal() done, defined at d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\deploy\\spec.py:346> exception=RuntimeError('Worker failed to start')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\tornado\\ioloop.py\", line 738, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\tornado\\ioloop.py\", line 762, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\deploy\\spec.py\", line 390, in _correct_state_internal\n",
      "    await asyncio.gather(*worker_futs)\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\asyncio\\tasks.py\", line 650, in _wrap_awaitable\n",
      "    return (yield from awaitable.__await__())\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\dask_cloudprovider\\aws\\ecs.py\", line 153, in _\n",
      "    await self.start()\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\dask_cloudprovider\\aws\\ecs.py\", line 263, in start\n",
      "    raise RuntimeError(\"%s failed to start\" % type(self).__name__)\n",
      "RuntimeError: Worker failed to start\n",
      "2023-11-28 19:48:38,843 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "2023-11-28 19:50:51,490 - distributed.deploy.cluster - WARNING - Failed to sync cluster info multiple times - perhaps there's a connection issue? Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\comm\\tcp.py\", line 547, in connect\n",
      "    stream = await self.client.connect(\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\tornado\\tcpclient.py\", line 279, in connect\n",
      "    af, addr, stream = await connector.start(connect_timeout=timeout)\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\asyncio\\tasks.py\", line 456, in wait_for\n",
      "    return fut.result()\n",
      "asyncio.exceptions.CancelledError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\comm\\core.py\", line 342, in connect\n",
      "    comm = await wait_for(\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\utils.py\", line 1940, in wait_for\n",
      "    return await asyncio.wait_for(fut, timeout)\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\asyncio\\tasks.py\", line 458, in wait_for\n",
      "    raise exceptions.TimeoutError() from exc\n",
      "asyncio.exceptions.TimeoutError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\deploy\\cluster.py\", line 165, in _sync_cluster_info\n",
      "    await self.scheduler_comm.set_metadata(\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\core.py\", line 1313, in send_recv_from_rpc\n",
      "    comm = await self.live_comm()\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\core.py\", line 1272, in live_comm\n",
      "    comm = await connect(\n",
      "  File \"d:\\Anaconda3\\envs\\solar-data-tools\\lib\\site-packages\\distributed\\comm\\core.py\", line 368, in connect\n",
      "    raise OSError(\n",
      "OSError: Timed out trying to connect to tcp://18.221.164.41:8786 after 30 s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _ in range(3):\n",
    "    results.append(run_pipeline(df))\n",
    "dask.compute(results)\n",
    "\n",
    "# There appears to be a faulty timeout error as it ran completely with correct results.\n",
    "# The error occurs when the worker should be spun down, so it makes sense it would timeout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('solar-data-tools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "563caecf4d2966d415acedaeb06ecd8f04c22f911b15c7586f55c5d4e031deb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
