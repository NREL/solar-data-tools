{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff26e055",
   "metadata": {},
   "source": [
    "# Summary of the efforts to parallelize within the pipeline so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c64a6",
   "metadata": {},
   "source": [
    "*Written by Xiao Ming*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a810550d",
   "metadata": {},
   "source": [
    "### 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70105ab",
   "metadata": {},
   "source": [
    "The existing data pipeline have a large number of operations. We want to test if the pipeline itself can be broken into individual Dask delayed functions and use multithreading/multiprocessing to speed up the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fa6d3",
   "metadata": {},
   "source": [
    "### 2. Understanding the subtasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ceb695",
   "metadata": {},
   "source": [
    "We begin with an analysis on subtasks dependencies by creating a directed acyclic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72804d55",
   "metadata": {},
   "source": [
    "Link to graph: https://www.figma.com/file/p3BJHBWEL8PQUXWGPYUxNf/Diagram-Basics-(Community)?type=whiteboard&node-id=0-1&t=oiyNPvAPb0DVlMFK-0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46358327",
   "metadata": {},
   "source": [
    "![title](img/dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881fae5",
   "metadata": {},
   "source": [
    "### 3. Identify parallelization targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dea60a",
   "metadata": {},
   "source": [
    "Note that in the diagram above, clipping_check, score_data_set, and capacity_clustering can all execute as soon as get_daily_flags finishes. If we can defined those functions as a Dask delayed function, we can then execute them in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01341ea",
   "metadata": {},
   "source": [
    "### 4. Refactor the code for the new parallelized pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb44f5",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "\n",
    "1. Entire pipeline refactored into Dask delayed functions\n",
    "2. Dask delayed function won't get executed until we call compute\n",
    "3. Data retrieval should also be a Dask delayed function\n",
    "4. From the above requirements as laid out by the clients, we need to pay attention to a few issues.\n",
    "\n",
    "\n",
    "Considerations/challenges:\n",
    "\n",
    "1. Because of the way Dask tasks are scheduled, Dask does not allow us to mutate the input, meaning that everytime we change an object attribute, that attribute should be passed into the function as input, copied, mutated, and then spat out as output. https://docs.dask.org/en/latest/delayed-best-practices.html#don-t-mutate-inputs\n",
    "2. For Dask multithreading/multiprocessing to have a meaningful impact on performance, the pipeline should be broken up into many small pieces. As can be seen from the diagram in part 2, if we only break apart the pipeline into the six predefined phases (preprocessing, cleaning, scoring, etc.), it is guaranteed that everything will be run serially, per the DAG above.\n",
    "3. Because of the first consideration (\"cannot mutate input/have to pass in everything the code changes\"), many of our delayed functions will have many inputs, each with a large size(They are objects and matrices). According to Dask best practices, Dask will calculate hash for each input. This means that Dask will need to calculate the hash for a lot of objects, arrays, and matrices, resulting in slowdown. https://docs.dask.org/en/latest/delayed-best-practices.html#avoid-repeatedly-putting-large-inputs-into-delayed-calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
