{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from solardatatools import DataHandler\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import dask\n",
    "from dask import delayed, compute, config\n",
    "from dask.distributed import Client, LocalCluster, performance_report\n",
    "import json\n",
    "import click\n",
    "import tempfile\n",
    "import csv\n",
    "from solardatatools.dataio import load_cassandra_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_csv_to_dh(file):\n",
    "    \"\"\"\n",
    "    Converts a local CSV file into a solar-data-tools DataHandler.\n",
    "    Parameters:\n",
    "    - file: Path to the CSV file.\n",
    "    Returns:\n",
    "    - A tuple of the file name and its corresponding DataHandler.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    # Convert index from int to datetime object\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    dh = DataHandler(df)\n",
    "    name = os.path.basename(file)\n",
    "    return (name, dh)\n",
    "\n",
    "\n",
    "def get_csvs_in_dir(folder_path):\n",
    "    \"\"\"\n",
    "    Gets the csvs in a directory.\n",
    "    Parameters:\n",
    "    - folder_path: Directory containing the csvs.\n",
    "    Returns:\n",
    "    - An array of the csv file paths.\n",
    "    \"\"\"\n",
    "    csvs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                csvs.append(file_path)\n",
    "    return csvs\n",
    "\n",
    "def run_job(data_result, track_times, local=True):\n",
    "    \"\"\"\n",
    "    Processes a single unit of data using DataHandler.\n",
    "    Parameters:\n",
    "    - data_result: Tuple of the file name and its corresponding DataHandler.\n",
    "    - track_times: Boolean to determine whether run times are added to the\n",
    "                   output.\n",
    "    Returns:\n",
    "    - A dictionary containing the name of the data and the processed report.\n",
    "    If there was an error with processing, only the name of the data is\n",
    "    returned.\n",
    "    \"\"\"\n",
    "    name = data_result[0]\n",
    "    data_handler = data_result[1]\n",
    "    column = None\n",
    "    if not local:\n",
    "        column = data_result[2]\n",
    "\n",
    "    try:\n",
    "        if local:\n",
    "            data_handler.run_pipeline()\n",
    "        else:\n",
    "            data_handler.run_pipeline(power_col=column)\n",
    "        report = data_handler.report(verbose=False, return_values=True)\n",
    "        report[\"name\"] = name\n",
    "        if track_times:\n",
    "            report[\"total_time\"] = data_handler.total_time\n",
    "    except:\n",
    "        report = {}\n",
    "        report[\"name\"] = name\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a672fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_local(filename, track_times=True):\n",
    "    \"\"\"\n",
    "    Generate the analysis task for a given local file. \n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Name of the local file\n",
    "    - track_times: Booleans to determine whether run times are added \n",
    "    to the output\n",
    "\n",
    "    Returns:\n",
    "    - A Dask delayed task object for the data analysis, which depends\n",
    "    on the ingest task. \n",
    "    \"\"\"\n",
    "    task_ingest = delayed(local_csv_to_dh)(filename)\n",
    "    task_analyze = delayed(run_job)(task_ingest, track_times)\n",
    "    return task_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks_directory(directory, track_times=True):\n",
    "    \"\"\"\n",
    "    Generate the analysis tasks for a given directory containing csv's. \n",
    "    \n",
    "    Parameters:\n",
    "    - directory: Path of the directory containing csv's\n",
    "    - track_times: Booleans to determine whether run times are added \n",
    "    to the output\n",
    "\n",
    "    Returns:\n",
    "    - A list of Dask delayed task objects for the data analysis, \n",
    "    each of which depends on an ingest task. \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for filename in get_csvs_in_dir(directory):\n",
    "        if \"log.csv\" not in filename:\n",
    "            result.append(generate_task_local(filename))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156999d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_reports(reports, filename=\"log.csv\"):\n",
    "    \"\"\"\n",
    "    Aggregate reports and write output to a csv file. \n",
    "    \n",
    "    Parameters:\n",
    "    - reports: list of reports. Each report is a dictionary. \n",
    "    - filename: name of the output csv\n",
    "    \"\"\"\n",
    "    if filename != \"\":\n",
    "        with open(filename, \"w\") as fp:\n",
    "            writer = csv.writer(fp)\n",
    "            header = []\n",
    "            for key in reports[0].keys():\n",
    "                header.append(key)\n",
    "            writer.writerow(header)\n",
    "            for r in reports:\n",
    "                writer.writerow(r.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4ea5e",
   "metadata": {},
   "source": [
    "# Visualize task graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5111d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = generate_tasks_directory(\"./\")\n",
    "aggregate_reports_task = delayed(write_reports)(obj_list, \"log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_reports_task.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f78239",
   "metadata": {},
   "source": [
    "# Execute Task Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks(task_list):\n",
    "    \"\"\"\n",
    "    Execute a list of tasks. \n",
    "    \n",
    "    NOTE: The Dask cluster should be \n",
    "    intialized before calling this function. \n",
    "    \n",
    "    Parameters:\n",
    "    - task_list: A list of dask delayed object\n",
    "\n",
    "    Returns:\n",
    "    - A list of reports from execution\n",
    "    \"\"\"\n",
    "    reports = compute(*task_list,)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker=4, n_workers=2)\n",
    "\n",
    "#reports = execute_tasks(obj_list)\n",
    "compute(aggregate_reports_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we need functions to generate tasks from remote sources\n",
    "\n",
    "\n",
    "# each node is a row in the record\n",
    "# identifier for that file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_site_to_dhs(site, track_times=True):\n",
    "    \"\"\"\n",
    "    Converts a remote database site into a solar-data-tools DataHandler.\n",
    "    Parameters:\n",
    "    - site: remote site.\n",
    "    Returns:\n",
    "    - A tuple of the unique identifier and its corresponding DataHandler.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    df = load_cassandra_data(site, cluster_ip=\"54.176.95.208\")\n",
    "    dh = DataHandler(df, convert_to_ts=True)\n",
    "    dh.data_frame_raw.index = dh.data_frame_raw.index.view(\"int\")\n",
    "    dh_keys = dh.keys\n",
    "    for key in dh_keys:\n",
    "        site = key[0][0]\n",
    "        site = site.strip()\n",
    "        system = key[0][1]\n",
    "        system = system.strip()\n",
    "        name = site + system\n",
    "        column = key[1]\n",
    "        task_analyze = delayed(run_job)((name, dh, column), track_times, False)\n",
    "        result.append(task_analyze)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks_remote_database(db_list):\n",
    "    \"\"\"\n",
    "    Generate the analysis tasks for remote database.\n",
    "\n",
    "    Parameters:\n",
    "    - db_list: Path of the directory containing a list of sites from remote database\n",
    "\n",
    "    Returns:\n",
    "    - A list of Dask delayed task objects for the data analysis,\n",
    "    each of which depends on an ingest task.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(db_list, \"r\") as file:\n",
    "        for site in file:\n",
    "            result.extend(remote_site_to_dhs(site))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88890b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_list = \"/Users/cclin/Documents/14798-SLAC Apache Beam Parallel Processing on AWS /resources/db_list\"\n",
    "obj_list = generate_tasks_remote_database(db_list)\n",
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1477edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2586b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
